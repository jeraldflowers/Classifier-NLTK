{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPMFsdnijfrvaelRRq8NZi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeraldflowers/Classifier-NLTK/blob/main/Accuracy_Classifier_Spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the Accuracy of the email Classifier"
      ],
      "metadata": {
        "id": "IPQNOjJG6hUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCh3_PRT6IIi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.collocations import *\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "!git clone https://github.com/pachocamacho1990/datasets\n",
        "!unzip datasets/email/plaintext/corpus1.zip\n",
        "!unzip datasets/email/plaintext/corpus2.zip\n",
        "!unzip datasets/email/plaintext/corpus3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for Load the Datasets"
      ],
      "metadata": {
        "id": "ixiajjT779aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# GET TEXT AND LABELS\n",
        "\n",
        "def get_text_labels_from_folders(folderBase, folderLabels):\n",
        "  data = []\n",
        "  labels = []\n",
        "  for folderLabel in folderLabels:\n",
        "    for file in os.listdir('{}/{}'. format(folderBase, folderLabel)):\n",
        "      with open('{}/{}/{}'.format(folderBase, folderLabel, file), encoding='latin-1') as f:\n",
        "        data.append(f.read())\n",
        "        labels.append(folderLabel)\n",
        "  return data, labels\n",
        "\n",
        "def set_label_num(label_str):\n",
        "  if label_str == 'spam':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "dataCorpus1, labelsCorpus1 = get_text_labels_from_folders('corpus1', ['spam', 'ham'])\n",
        "dataCorpus2, labelsCorpus2 = get_text_labels_from_folders('corpus2', ['spam', 'ham'])\n",
        "dataCorpus3, labelsCorpus3 = get_text_labels_from_folders('corpus3', ['spam', 'ham'])\n",
        "\n",
        "data = dataCorpus1 + labelsCorpus2 + labelsCorpus3\n",
        "labels = labelsCorpus1 +  labelsCorpus2 + labelsCorpus3\n",
        "\n",
        "dataframe = pd.DataFrame({'text': data,\n",
        "                          'labels': labels})\n",
        "\n",
        "dataframe = dataframe.sample(frac = 1)\n",
        "dataframe['tokens'] = dataframe['text'].apply(lambda x: word_tokenize(x))\n",
        "dataframe['labels_num'] = dataframe['labels'].apply(lambda x: set_label_num(x))"
      ],
      "metadata": {
        "id": "5IryLfBg7l38"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Filter Words"
      ],
      "metadata": {
        "id": "oUKy6RdBAtX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_words_by_threshold(text_tokenized, threshold=3):\n",
        "  words = []\n",
        "  words = [word for word in text_tokenized if len(word) > threshold]\n",
        "  return words\n",
        "\n",
        "def get_n_grams_collocations_from_words(words, freq_filter=10, n_best=10, n_gram_measure=nltk.collocations.BigramAssocMeasures()):\n",
        "  finder = BigramCollocationFinder.from_words(words)\n",
        "  finder.apply_freq_filter(freq_filter)\n",
        "  email_spam_collocations = finder.nbest(n_gram_measure.pmi, n_best)\n",
        "  return email_spam_collocations\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAeBjQYfAiwX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Most Common Collocations and Words in the Spam Datasets"
      ],
      "metadata": {
        "id": "eu80lS_5Hmjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, spamCorpus1 = get_text_labels_from_folders('corpus1', ['spam'])\n",
        "_, spamCorpus2 = get_text_labels_from_folders('corpus2', ['spam'])\n",
        "_, spamCorpus3 = get_text_labels_from_folders('corpus3', ['spam'])\n",
        "\n",
        "spamCorpuses = spamCorpus1 + spamCorpus2 + spamCorpus3\n",
        "\n",
        "filtered_words = []\n",
        "\n",
        "for text in spamCorpuses:\n",
        "  filtered_words += filter_words_by_threshold(word_tokenize(text))\n",
        "filtered_words  \n",
        "\n",
        "email_spam_collocations = get_n_grams_collocations_from_words(filtered_words, 120, 40)\n",
        "all_spam_words = nltk.FreqDist([w for w in filtered_words])\n",
        "top_spam_words = all_spam_words.most_common(200)"
      ],
      "metadata": {
        "id": "g-rfRw-6Hpah"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Most Common Collocations and Words in the Ham Datasets"
      ],
      "metadata": {
        "id": "nM9viPu9Duwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, hamCorpus1 = get_text_labels_from_folders('corpus1', ['ham'])\n",
        "_, hamCorpus2 = get_text_labels_from_folders('corpus2', ['ham'])\n",
        "_, hamCorpus3 = get_text_labels_from_folders('corpus3', ['ham'])\n",
        "\n",
        "hamCorpuses = hamCorpus1 + hamCorpus2 + hamCorpus3\n",
        "\n",
        "filtered_words = []\n",
        "\n",
        "for text in hamCorpuses:\n",
        "  filtered_words += filter_words_by_threshold(word_tokenize(text))\n",
        "filtered_words\n",
        "\n",
        "email_ham_collocations = get_n_grams_collocations_from_words(filtered_words, 120, 40)\n",
        "all_ham_words = nltk.FreqDist([w for w in filtered_words])\n",
        "top_ham_words = all_ham_words.most_common(200)"
      ],
      "metadata": {
        "id": "oMqrCm8tDwuW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Repeated Words of Most Common Words in Spam and Ham"
      ],
      "metadata": {
        "id": "18QVZid5FJzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_ham_words_iterator = top_ham_words\n",
        "\n",
        "for word in top_ham_words_iterator:\n",
        "  if word in top_ham_words and word in top_spam_words:\n",
        "    top_ham_words.remove(word)\n",
        "    top_spam_words.remove(word)"
      ],
      "metadata": {
        "id": "Q13gwY4hFeEw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Most Commons Words of all Dataset"
      ],
      "metadata": {
        "id": "JxaHA2j8Imw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words = []\n",
        "\n",
        "for text in data:\n",
        "  filtered_words += filter_words_by_threshold(word_tokenize(text))\n",
        "filtered_words\n",
        "\n",
        "all_words = nltk.FreqDist([w for w in filtered_words])\n",
        "top_words = all_words.most_common(200)\n",
        "top_words"
      ],
      "metadata": {
        "id": "gxpK68NcIt_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Attributes"
      ],
      "metadata": {
        "id": "ZwopaB0NKL0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def document_attributes(document):\n",
        "  document_words = set(document)\n",
        "  atrib = {}\n",
        "  for word in top_words:\n",
        "    atrib['contains({})'.format(word)] = (word in document_words)\n",
        "\n",
        "  for word in top_spam_words:\n",
        "    atrib['contains_spam_words({})'.format(word)] = (word in document_words)\n",
        "\n",
        "  for word in top_ham_words:\n",
        "    atrib['contains_ham_words({})'.format(word)] = (word in document_words)\n",
        "\n",
        "  for word in document_words:\n",
        "    has_spam_word = False\n",
        "    has_ham_word = False\n",
        "    for bigram_position_0, bigram_position_1 in email_spam_collocations:\n",
        "      if word == bigram_position_0 or word == bigram_position_1:\n",
        "        has_spam_word = True\n",
        "        break\n",
        "    for bigram_position_0, bigram_position_1 in email_ham_collocations:\n",
        "      if word == bigram_position_0 or word == bigram_position_1:\n",
        "        has_ham_word = True\n",
        "        break\n",
        "\n",
        "  atrib['spam_word({})'.format(word)] = has_spam_word\n",
        "  atrib['ham_word({})'.format(word)] = has_ham_word\n",
        "\n",
        "  filtered_words = filter_words_by_threshold(document)\n",
        "  bigrams = get_n_grams_collocations_from_words(filtered_words, n_best=10, freq_filter=5)\n",
        "\n",
        "  for i in range(len(bigrams)):\n",
        "    atrib['bigram_collocation({})'.format(i)] = bigrams[i]\n",
        "\n",
        "  return atrib"
      ],
      "metadata": {
        "id": "yOCqSA6XKUTp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate Training and Test Dataset"
      ],
      "metadata": {
        "id": "70U9TY1yRlP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fset = [(document_attributes(text), labels) for text, labels in zip(dataframe['tokens'], dataframe['labels_num'].values)]\n",
        "\n",
        "random.shuffle(fset)\n",
        "print(len(fset))\n",
        "\n",
        "train, test = fset[:13078], fset[13078:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-B4vgKORo6I",
        "outputId": "d918b089-1ac2-4473-c212-fae9374e4515"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Calculate Accuracy"
      ],
      "metadata": {
        "id": "yJJs5V6dYxbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(train)\n",
        "\n",
        "print(nltk.classify.accuracy(classifier, test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZvdPjtsYxFN",
        "outputId": "d9e7038f-81ae-4ec6-a0a5-8eb4f37472e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9400428265524625\n"
          ]
        }
      ]
    }
  ]
}